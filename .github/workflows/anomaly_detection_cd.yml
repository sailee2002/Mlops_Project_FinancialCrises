# name: Anomaly Detection - Continuous Deployment

# on:
#   push:
#     branches:
#       - main
#       - sush_new
#     paths:
#       - "src/eda/eda.py"
#       - "src/labeling/**"
#       - "src/models/train_anomaly_detection.py"
#       - "configs/**"
#   workflow_dispatch:

# env:
#   PYTHON_VERSION: "3.10"
#   GCP_BUCKET: "mlops-financial-stress-data"
#   GCP_PROJECT: "ninth-iris-422916-f2"

# jobs:
#   run-tests:
#     runs-on: ubuntu-latest
    
#     steps:
#       - uses: actions/checkout@v4
      
#       - uses: actions/setup-python@v4
#         with:
#           python-version: ${{ env.PYTHON_VERSION }}

#       - name: Free disk space
#         run: |
#           sudo rm -rf /usr/share/dotnet
#           sudo rm -rf /usr/local/lib/android
#           sudo rm -rf /opt/ghc

#       - name: Install dependencies
#         run: |
#           pip install --upgrade pip
#           pip install -r requirements.txt
#           pip install pytest
      
#       - name: Run tests
#         run: |
#           if [ -d "tests" ]; then
#             pytest tests/ -k "anomaly" || echo "No anomaly tests found"
#           else
#             echo "No tests directory found - skipping"
#           fi
#         continue-on-error: true

#   # Complete data pipeline: EDA â†’ Thresholds â†’ Snorkel Labeling
#   run-data-pipeline:
#     runs-on: ubuntu-latest
#     needs: [run-tests]
    
#     steps:
#       - uses: actions/checkout@v4
      
#       - uses: actions/setup-python@v4
#         with:
#           python-version: ${{ env.PYTHON_VERSION }}
      
#       - name: Authenticate to Google Cloud
#         uses: google-github-actions/auth@v2
#         with:
#           credentials_json: ${{ secrets.GCP_SA_KEY }}
      
#       - name: Set up Cloud SDK
#         uses: google-github-actions/setup-gcloud@v2
      
#       - name: Free disk space
#         run: |
#           sudo rm -rf /usr/share/dotnet
#           sudo rm -rf /usr/local/lib/android
#           sudo rm -rf /opt/ghc

#       - name: Install dependencies
#         run: |
#           pip install --upgrade pip
#           pip install -r requirements.txt
      
#       # STEP 1: Download features_engineered.csv from GCS
#       - name: Step 1 - Download features_engineered.csv
#         run: |
#           echo "ğŸ“¥ STEP 1: Downloading features_engineered.csv from GCS..."
#           mkdir -p data/processed data/eda data/labeled
          
#           gsutil cp gs://${{ env.GCP_BUCKET }}/data/processed/features_engineered.csv data/processed/ || {
#             echo "âŒ features_engineered.csv not found in GCS"
#             exit 1
#           }
          
#           echo "âœ… features_engineered.csv downloaded"
#           ls -lh data/processed/features_engineered.csv
      
#       # STEP 2: Run EDA
#       - name: Step 2 - Run EDA analysis
#         run: |
#           echo "ğŸ“Š STEP 2: Running EDA analysis..."
          
#           if [ -f "src/eda/eda.py" ]; then
#             python src/eda/eda.py
#             echo "âœ… EDA completed"
            
#             # Check outputs
#             if [ -d "outputs/eda" ]; then
#               echo "EDA outputs created:"
#               find outputs/eda -type f -name "*.png" -o -name "*.csv" -o -name "*.txt" | head -10
#             fi
#           else
#             echo "âŒ EDA script not found at src/eda/eda.py"
#             exit 1
#           fi
      
#       # STEP 3: Extract Auto Thresholds
#       - name: Step 3 - Extract auto thresholds
#         run: |
#           echo "ğŸ¯ STEP 3: Extracting auto thresholds..."
          
#           if [ -f "src/labeling/auto_threshold_extractor.py" ]; then
#             python src/labeling/auto_threshold_extractor.py
#             echo "âœ… Auto threshold extraction completed"
            
#             # Check threshold outputs
#             if [ -f "outputs/snorkel/data/extracted_thresholds.json" ]; then
#               echo "Thresholds extracted:"
#               ls -lh outputs/snorkel/data/extracted_thresholds.*
#             fi
#           else
#             echo "âŒ Auto threshold script not found"
#             exit 1
#           fi
      
#       # STEP 4: Run Snorkel Labeling
#       - name: Step 4 - Run Snorkel labeling pipeline
#         run: |
#           echo "ğŸ·ï¸  STEP 4: Running Snorkel labeling pipeline..."
          
#           if [ -f "src/labeling/snorkel_pipeline.py" ]; then
#             python src/labeling/snorkel_pipeline.py
#             echo "âœ… Snorkel labeling completed"
            
#             # Verify labeled data
#             if [ -f "outputs/snorkel/data/snorkel_labeled_data.csv" ]; then
#               echo "âœ… Labeled data created:"
#               ls -lh outputs/snorkel/data/snorkel_labeled_data.csv
#               echo ""
#               echo "Data preview:"
#               head -n 5 outputs/snorkel/data/snorkel_labeled_data.csv
#             else
#               echo "âŒ Labeled data not found"
#               exit 1
#             fi
#           else
#             echo "âŒ Snorkel pipeline script not found"
#             exit 1
#           fi
      
#       # Upload all pipeline outputs to GCS
#       - name: Upload pipeline outputs to GCS
#         run: |
#           echo "ğŸ“¤ Uploading all pipeline outputs to GCS..."
          
#           gcloud config set project ${{ env.GCP_PROJECT }}
          
#           # Upload EDA outputs
#           if [ -d "outputs/eda" ]; then
#             echo "Uploading EDA outputs..."
#             gsutil -m rsync -r outputs/eda/ gs://${{ env.GCP_BUCKET }}/outputs/eda/
#             echo "âœ… EDA outputs uploaded"
#           fi
          
#           # Upload Snorkel outputs (thresholds + labeled data)
#           if [ -d "outputs/snorkel" ]; then
#             echo "Uploading Snorkel outputs..."
#             gsutil -m rsync -r outputs/snorkel/ gs://${{ env.GCP_BUCKET }}/outputs/snorkel/
#             echo "âœ… Snorkel outputs uploaded"
#           fi
          
#           echo ""
#           echo "âœ… All pipeline outputs uploaded successfully!"
#         continue-on-error: true
      
#       # Upload labeled data as artifact for next job
#       - name: Upload labeled data as artifact
#         uses: actions/upload-artifact@v4
#         with:
#           name: labeled-data
#           path: outputs/snorkel/data/snorkel_labeled_data.csv
#           retention-days: 7

#   # Train anomaly detection models
#   train-anomaly-models:
#     runs-on: ubuntu-latest
#     needs: [run-tests, run-data-pipeline]
    
#     steps:
#       - uses: actions/checkout@v4
      
#       - uses: actions/setup-python@v4
#         with:
#           python-version: ${{ env.PYTHON_VERSION }}
      
#       - name: Authenticate to Google Cloud
#         uses: google-github-actions/auth@v2
#         with:
#           credentials_json: ${{ secrets.GCP_SA_KEY }}
      
#       - name: Set up Cloud SDK
#         uses: google-github-actions/setup-gcloud@v2
      
#       - name: Free disk space
#         run: |
#           sudo rm -rf /usr/share/dotnet
#           sudo rm -rf /usr/local/lib/android
#           sudo rm -rf /opt/ghc
          
#       - name: Install dependencies
#         run: |
#           pip install --upgrade pip
#           pip install -r requirements.txt
      
#       # Download labeled data from previous job
#       - name: Download labeled data artifact
#         uses: actions/download-artifact@v4
#         with:
#           name: labeled-data
#           path: data/labeled
#         continue-on-error: true
      
#       # Fallback: Download from GCS if artifact fails
#       - name: Fallback - Download labeled data from GCS
#         run: |
#           if [ ! -f "data/labeled/snorkel_labeled_data.csv" ]; then
#             echo "ğŸ“¥ Downloading labeled data from GCS..."
#             mkdir -p data/labeled
#             gsutil cp gs://${{ env.GCP_BUCKET }}/outputs/snorkel/data/snorkel_labeled_data.csv data/labeled/ || {
#               echo "âŒ Labeled data not found in GCS"
#               exit 1
#             }
#             echo "âœ… Labeled data downloaded from GCS"
#           fi
      
#       - name: Verify labeled data
#         id: check_data
#         run: |
#           if [ -f "data/labeled/snorkel_labeled_data.csv" ]; then
#             echo "data_exists=true" >> $GITHUB_OUTPUT
#             echo "âœ… Labeled data verified"
#             ls -lh data/labeled/
#             echo ""
#             echo "Sample data:"
#             head -n 3 data/labeled/snorkel_labeled_data.csv
#           else
#             echo "data_exists=false" >> $GITHUB_OUTPUT
#             echo "âŒ Labeled data not found"
#             exit 1
#           fi
      
#       # Create output directories
#       - name: Create output directories
#         if: steps.check_data.outputs.data_exists == 'true'
#         run: |
#           mkdir -p outputs/models/{results,plots,reports}
#           mkdir -p models/anomaly_detection
#           mkdir -p logs
#           echo "âœ… Output directories created"
      
#       # STEP 5: Train all anomaly detection models
#       - name: Step 5 - Train anomaly detection models
#         if: steps.check_data.outputs.data_exists == 'true'
#         run: |
#           echo "ğŸ¤– STEP 5: Training anomaly detection models..."
#           echo ""
#           echo "Models to train:"
#           echo "  - Isolation Forest"
#           echo "  - Local Outlier Factor (LOF)"
#           echo "  - One-Class SVM"
#           echo "  - DBSCAN (if enabled)"
#           echo ""
          
#           # Run complete training pipeline
#           python src/models/train_anomaly_detection.py
          
#           echo ""
#           echo "âœ… Model training completed"
#         env:
#           MLFLOW_TRACKING_URI: "file:./mlruns"
      
#       - name: Check training results
#         if: steps.check_data.outputs.data_exists == 'true'
#         run: |
#           echo "ğŸ“Š Checking Training Results..."
#           echo ""
          
#           # Check models
#           if [ -d "models/anomaly_detection" ]; then
#             echo "âœ… Models trained:"
#             find models/anomaly_detection -name "*.pkl" -type f | while read model; do
#               echo "  - $model ($(du -h "$model" | cut -f1))"
#             done
#             echo ""
#           else
#             echo "âš ï¸  No models directory found"
#           fi
          
#           # Check reports
#           if [ -f "outputs/models/reports/comprehensive_training_report.txt" ]; then
#             echo "âœ… Training report generated"
#             echo ""
#             echo "=== REPORT PREVIEW ==="
#             head -n 60 outputs/models/reports/comprehensive_training_report.txt
#             echo ""
#           else
#             echo "âš ï¸  Training report not found"
#           fi
          
#           # Check sensitivity analysis
#           if [ -f "outputs/models/plots/sensitivity_analysis.png" ]; then
#             echo "âœ… Sensitivity analysis plot created"
#           fi
          
#           # Check model comparison
#           if [ -f "outputs/models/plots/model_comparison.png" ]; then
#             echo "âœ… Model comparison plot created"
#           fi
#         continue-on-error: true
      
#       # Upload all training outputs to GCS
#       - name: Upload models and outputs to GCS
#         if: steps.check_data.outputs.data_exists == 'true'
#         run: |
#           echo "ğŸ“¤ Uploading trained models and outputs to GCS..."
#           echo ""
          
#           gcloud config set project ${{ env.GCP_PROJECT }}
          
#           # Upload trained models
#           if [ -d "models/anomaly_detection" ]; then
#             echo "Uploading models to gs://${{ env.GCP_BUCKET }}/models/model3/..."
#             gsutil -m rsync -r models/anomaly_detection/ gs://${{ env.GCP_BUCKET }}/models/model3/
#             echo "âœ… Models uploaded"
#             echo ""
#           fi
          
#           # Upload analysis outputs (plots, reports, results)
#           if [ -d "outputs/models" ]; then
#             echo "Uploading outputs to gs://${{ env.GCP_BUCKET }}/outputs/model3/..."
#             gsutil -m rsync -r outputs/models/ gs://${{ env.GCP_BUCKET }}/outputs/model3/
#             echo "âœ… Outputs uploaded"
#             echo ""
#           fi
          
#           # Upload MLflow logs
#           if [ -d "mlruns" ]; then
#             echo "Uploading MLflow logs..."
#             gsutil -m rsync -r mlruns/ gs://${{ env.GCP_BUCKET }}/mlruns/model3/
#             echo "âœ… MLflow logs uploaded"
#             echo ""
#           fi
          
#           echo "=========================================="
#           echo "âœ… All artifacts uploaded to GCS!"
#           echo "=========================================="
#           echo ""
#           echo "GCS Locations:"
#           echo "  Bucket:     gs://${{ env.GCP_BUCKET }}/"
#           echo "  Models:     gs://${{ env.GCP_BUCKET }}/models/model3/"
#           echo "  Outputs:    gs://${{ env.GCP_BUCKET }}/outputs/model3/"
#           echo "  EDA:        gs://${{ env.GCP_BUCKET }}/outputs/eda/"
#           echo "  Snorkel:    gs://${{ env.GCP_BUCKET }}/outputs/snorkel/"
#           echo "  MLflow:     gs://${{ env.GCP_BUCKET }}/mlruns/model3/"
#         continue-on-error: true
      
#       - name: Upload training artifacts to GitHub
#         if: steps.check_data.outputs.data_exists == 'true'
#         uses: actions/upload-artifact@v4
#         with:
#           name: anomaly-detection-models
#           path: |
#             models/anomaly_detection/**/*.pkl
#             outputs/models/reports/*.txt
#             outputs/models/plots/*.png
#             outputs/models/results/*.csv
#           retention-days: 30
#         continue-on-error: true

#   # Validate models and check deployment readiness
#   validate-models:
#     runs-on: ubuntu-latest
#     needs: [train-anomaly-models]
#     if: always()
    
#     steps:
#       - uses: actions/checkout@v4
      
#       - uses: actions/setup-python@v4
#         with:
#           python-version: ${{ env.PYTHON_VERSION }}
      
#       - name: Authenticate to Google Cloud
#         uses: google-github-actions/auth@v2
#         with:
#           credentials_json: ${{ secrets.GCP_SA_KEY }}
      
#       - name: Set up Cloud SDK
#         uses: google-github-actions/setup-gcloud@v2
      
#       - name: Download training artifacts
#         uses: actions/download-artifact@v4
#         with:
#           name: anomaly-detection-models
#           path: artifacts
#         continue-on-error: true
      
#       - name: Validate model performance
#         run: |
#           echo "ğŸ” Validating Model Performance..."
#           echo ""
          
#           if [ -f "artifacts/outputs/models/reports/comprehensive_training_report.txt" ]; then
#             echo "âœ… Training report found"
#             echo ""
            
#             # Extract best model info
#             echo "=== BEST MODEL SELECTED ==="
#             grep -A 10 "BEST MODEL SELECTED" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
#             echo ""
            
#             # Extract validation results
#             echo "=== VALIDATION STATUS ==="
#             grep -A 15 "VALIDATION RESULTS" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
#             echo ""
            
#             # Extract bias detection results
#             echo "=== BIAS DETECTION ==="
#             grep -A 8 "BIAS DETECTION SUMMARY" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
#             echo ""
            
#           else
#             echo "âš ï¸  Training report not found in artifacts"
#           fi
#         continue-on-error: true
      
#       - name: Check deployment readiness in GCS
#         run: |
#           echo ""
#           echo "ğŸš€ Checking Deployment Readiness in GCS..."
#           echo ""
          
#           gcloud config set project ${{ env.GCP_PROJECT }}
          
#           # Check for models
#           echo "Checking for trained models..."
#           if gsutil ls gs://${{ env.GCP_BUCKET }}/models/model3/ > /dev/null 2>&1; then
#             echo "âœ… Model directory exists in GCS"
            
#             # Count model files
#             MODEL_COUNT=$(gsutil ls gs://${{ env.GCP_BUCKET }}/models/model3/**/*.pkl 2>/dev/null | wc -l)
            
#             if [ "$MODEL_COUNT" -gt 0 ]; then
#               echo "âœ… Found $MODEL_COUNT model files"
#               echo ""
#               echo "Models in GCS:"
#               gsutil ls gs://${{ env.GCP_BUCKET }}/models/model3/*/model.pkl | sed 's|gs://mlops-financial-stress-data/models/model3/||' | sed 's|/model.pkl||'
#               echo ""
#               echo "âœ… Models are READY for deployment"
#             else
#               echo "âŒ No model files found"
#               echo "âŒ Models are NOT ready for deployment"
#             fi
#           else
#             echo "âŒ Model directory not found in GCS"
#             echo "âŒ Models are NOT ready for deployment"
#           fi
          
#           # Check for outputs
#           echo ""
#           echo "Checking for analysis outputs..."
#           if gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/ > /dev/null 2>&1; then
#             echo "âœ… Analysis outputs directory exists"
            
#             # List key outputs
#             echo ""
#             echo "Available outputs:"
#             gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.png 2>/dev/null | wc -l | xargs echo "  Plots:"
#             gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.csv 2>/dev/null | wc -l | xargs echo "  Results:"
#             gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.txt 2>/dev/null | wc -l | xargs echo "  Reports:"
#           fi
#         continue-on-error: true

#   # Pipeline summary
#   pipeline-summary:
#     runs-on: ubuntu-latest
#     needs: [run-tests, run-data-pipeline, train-anomaly-models, validate-models]
#     if: always()
    
#     steps:
#       - name: Display comprehensive pipeline summary
#         run: |
#           echo "=========================================="
#           echo "ANOMALY DETECTION CD PIPELINE SUMMARY"
#           echo "=========================================="
#           echo ""
#           echo "Pipeline Execution Status:"
#           echo "  Tests:              ${{ needs.run-tests.result }}"
#           echo "  Data Pipeline:      ${{ needs.run-data-pipeline.result }}"
#           echo "  Model Training:     ${{ needs.train-anomaly-models.result }}"
#           echo "  Model Validation:   ${{ needs.validate-models.result }}"
#           echo ""
          
#           if [ "${{ needs.train-anomaly-models.result }}" == "success" ]; then
#             echo "âœ… =========================================="
#             echo "âœ… PIPELINE COMPLETED SUCCESSFULLY!"
#             echo "âœ… =========================================="
#             echo ""
#             echo "ğŸ“Š Complete Pipeline Executed:"
#             echo ""
#             echo "  Step 1: âœ… Downloaded features_engineered.csv from GCS"
#             echo "  Step 2: âœ… Ran EDA analysis (outputs/eda/)"
#             echo "  Step 3: âœ… Extracted auto thresholds (outputs/snorkel/data/)"
#             echo "  Step 4: âœ… Generated Snorkel labels (outputs/snorkel/data/)"
#             echo "  Step 5: âœ… Trained anomaly detection models"
#             echo ""
#             echo "ğŸ¯ Models Trained:"
#             echo "  âœ“ Isolation Forest"
#             echo "  âœ“ Local Outlier Factor (LOF)"
#             echo "  âœ“ One-Class SVM"
#             echo "  âœ“ DBSCAN (if enabled)"
#             echo ""
#             echo "ğŸ“ˆ Analysis Completed:"
#             echo "  âœ“ Hyperparameter sensitivity analysis"
#             echo "  âœ“ Bias detection (sector slicing)"
#             echo "  âœ“ SHAP feature importance"
#             echo "  âœ“ Model comparison and selection"
#             echo ""
#             echo "ğŸ“¦ All Outputs Uploaded to GCS:"
#             echo ""
#             echo "  gs://mlops-financial-stress-data/"
#             echo "  â”œâ”€â”€ data/"
#             echo "  â”‚   â””â”€â”€ processed/features_engineered.csv"
#             echo "  â”œâ”€â”€ models/"
#             echo "  â”‚   â””â”€â”€ model3/                    â† Trained models"
#             echo "  â”‚       â”œâ”€â”€ Isolation_Forest/"
#             echo "  â”‚       â”œâ”€â”€ LOF/"
#             echo "  â”‚       â”œâ”€â”€ One_Class_SVM/"
#             echo "  â”‚       â””â”€â”€ DBSCAN/"
#             echo "  â”œâ”€â”€ outputs/"
#             echo "  â”‚   â”œâ”€â”€ eda/                       â† EDA analysis"
#             echo "  â”‚   â”œâ”€â”€ snorkel/                   â† Labels & thresholds"
#             echo "  â”‚   â””â”€â”€ model3/                    â† Training results"
#             echo "  â”‚       â”œâ”€â”€ plots/                 â† Visualizations"
#             echo "  â”‚       â”œâ”€â”€ results/               â† Performance metrics"
#             echo "  â”‚       â””â”€â”€ reports/               â† Summary reports"
#             echo "  â””â”€â”€ mlruns/"
#             echo "      â””â”€â”€ model3/                    â† MLflow logs"
#             echo ""
#             echo "ğŸ”— Quick Access Links:"
#             echo "  View all models:  gsutil ls gs://mlops-financial-stress-data/models/model3/"
#             echo "  View reports:     gsutil cat gs://mlops-financial-stress-data/outputs/model3/reports/comprehensive_training_report.txt"
#             echo "  Download plots:   gsutil -m cp -r gs://mlops-financial-stress-data/outputs/model3/plots/ ./local-plots/"
#             echo ""
#             echo "=========================================="
#             echo "âœ… READY FOR DEPLOYMENT!"
#             echo "=========================================="
#           else
#             echo "âš ï¸  =========================================="
#             echo "âš ï¸  PIPELINE COMPLETED WITH ISSUES"
#             echo "âš ï¸  =========================================="
#             echo ""
#             echo "Check individual job logs for details:"
#             echo "  - Data Pipeline: ${{ needs.run-data-pipeline.result }}"
#             echo "  - Model Training: ${{ needs.train-anomaly-models.result }}"
#             echo "  - Validation: ${{ needs.validate-models.result }}"
#             echo ""
#             echo "Review the workflow run for error messages"
#           fi
#           echo ""
      
#       - name: Set status annotations
#         run: |
#           if [ "${{ needs.train-anomaly-models.result }}" == "success" ]; then
#             echo "::notice title=Pipeline Complete::Anomaly detection pipeline executed successfully"
#             echo "::notice title=Models Ready::Models available at gs://mlops-financial-stress-data/models/model3/"
#             echo "::notice title=Full Pipeline::EDA â†’ Thresholds â†’ Snorkel â†’ Training completed"
#           else
#             echo "::warning title=Pipeline Issues::Check individual job logs for details"
#           fi    

name: Anomaly Detection - Continuous Deployment

on:
  push:
    branches:
      - main
      - sush_new
    paths:
      - "src/eda/eda.py"
      - "src/labeling/**"
      - "src/models/train_anomaly_detection.py"
      - "configs/**"
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.10"
  GCP_BUCKET: "mlops-financial-stress-data"
  GCP_PROJECT: "ninth-iris-422916-f2"

jobs:
  run-tests:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Run tests
        run: |
          if [ -d "tests" ]; then
            pytest tests/ -k "anomaly" || echo "No anomaly tests found"
          else
            echo "No tests directory found - skipping"
          fi
        continue-on-error: true

  # Complete data pipeline: EDA â†’ Thresholds â†’ Snorkel Labeling
  run-data-pipeline:
    runs-on: ubuntu-latest
    needs: [run-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      # STEP 1: Download features_engineered.csv from GCS
      - name: Step 1 - Download features_engineered.csv
        run: |
          echo "ğŸ“¥ STEP 1: Downloading features_engineered.csv from GCS..."
          mkdir -p data/processed data/eda data/labeled
          
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/processed/features_engineered.csv data/processed/ || {
            echo "âŒ features_engineered.csv not found in GCS"
            exit 1
          }
          
          echo "âœ… features_engineered.csv downloaded"
          ls -lh data/processed/features_engineered.csv
      
      # STEP 2: Run EDA
      - name: Step 2 - Run EDA analysis
        run: |
          echo "ğŸ“Š STEP 2: Running EDA analysis..."
          
          if [ -f "src/eda/eda.py" ]; then
            python src/eda/eda.py
            echo "âœ… EDA completed"
            
            # Check outputs
            if [ -d "outputs/eda" ]; then
              echo "EDA outputs created:"
              find outputs/eda -type f -name "*.png" -o -name "*.csv" -o -name "*.txt" | head -10
            fi
          else
            echo "âŒ EDA script not found at src/eda/eda.py"
            exit 1
          fi
      
      # STEP 3: Extract Auto Thresholds
      - name: Step 3 - Extract auto thresholds
        run: |
          echo "ğŸ¯ STEP 3: Extracting auto thresholds..."
          
          if [ -f "src/labeling/auto_threshold_extractor.py" ]; then
            python src/labeling/auto_threshold_extractor.py
            echo "âœ… Auto threshold extraction completed"
            
            # Check threshold outputs
            if [ -f "outputs/snorkel/data/extracted_thresholds.json" ]; then
              echo "Thresholds extracted:"
              ls -lh outputs/snorkel/data/extracted_thresholds.*
            fi
          else
            echo "âŒ Auto threshold script not found"
            exit 1
          fi
      
      # STEP 4: Run Snorkel Labeling
      - name: Step 4 - Run Snorkel labeling pipeline
        run: |
          echo "ğŸ·ï¸  STEP 4: Running Snorkel labeling pipeline..."
          
          if [ -f "src/labeling/snorkel_pipeline.py" ]; then
            python src/labeling/snorkel_pipeline.py
            echo "âœ… Snorkel labeling completed"
            
            # Verify labeled data
            if [ -f "outputs/snorkel/data/snorkel_labeled_data.csv" ]; then
              echo "âœ… Labeled data created:"
              ls -lh outputs/snorkel/data/snorkel_labeled_data.csv
              echo ""
              echo "Data preview:"
              head -n 5 outputs/snorkel/data/snorkel_labeled_data.csv
            else
              echo "âŒ Labeled data not found"
              exit 1
            fi
          else
            echo "âŒ Snorkel pipeline script not found"
            exit 1
          fi
      
      # Upload all pipeline outputs to GCS
      - name: Upload pipeline outputs to GCS
        run: |
          echo "ğŸ“¤ Uploading all pipeline outputs to GCS..."
          
          # Upload EDA outputs
          if [ -d "outputs/eda" ]; then
            echo "Uploading EDA outputs..."
            gsutil -m rsync -r outputs/eda/ gs://${{ env.GCP_BUCKET }}/outputs/eda/
            echo "âœ… EDA outputs uploaded"
          fi
          
          # Upload Snorkel outputs (thresholds + labeled data)
          if [ -d "outputs/snorkel" ]; then
            echo "Uploading Snorkel outputs..."
            gsutil -m rsync -r outputs/snorkel/ gs://${{ env.GCP_BUCKET }}/outputs/snorkel/
            echo "âœ… Snorkel outputs uploaded"
          fi
          
          echo ""
          echo "âœ… All pipeline outputs uploaded successfully!"
        continue-on-error: true
      
      # Upload labeled data as artifact for next job
      - name: Upload labeled data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: labeled-data
          path: outputs/snorkel/data/snorkel_labeled_data.csv
          retention-days: 7

  # Train anomaly detection models
  train-anomaly-models:
    runs-on: ubuntu-latest
    needs: [run-tests, run-data-pipeline]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      # Download labeled data from previous job
      - name: Download labeled data artifact
        uses: actions/download-artifact@v4
        with:
          name: labeled-data
          path: outputs/snorkel/data
        continue-on-error: true
      
      # Fallback: Download from GCS if artifact fails
      - name: Fallback - Download labeled data from GCS
        run: |
          if [ ! -f "outputs/snorkel/data/snorkel_labeled_data.csv" ]; then
            echo "ğŸ“¥ Downloading labeled data from GCS..."
            mkdir -p outputs/snorkel/data
            gsutil cp gs://${{ env.GCP_BUCKET }}/outputs/snorkel/data/snorkel_labeled_data.csv outputs/snorkel/data/ || {
              echo "âŒ Labeled data not found in GCS"
              exit 1
            }
            echo "âœ… Labeled data downloaded from GCS"
          fi
      
      - name: Verify labeled data
        id: check_data
        run: |
          if [ -f "outputs/snorkel/data/snorkel_labeled_data.csv" ]; then
            echo "data_exists=true" >> $GITHUB_OUTPUT
            echo "âœ… Labeled data verified"
            ls -lh outputs/snorkel/data/
            echo ""
            echo "Sample data:"
            head -n 3 outputs/snorkel/data/snorkel_labeled_data.csv
          else
            echo "data_exists=false" >> $GITHUB_OUTPUT
            echo "âŒ Labeled data not found"
            exit 1
          fi
      
      # Create output directories
      - name: Create output directories
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          mkdir -p outputs/models/{results,plots,reports}
          mkdir -p models/anomaly_detection
          mkdir -p logs
          echo "âœ… Output directories created"
      
      # STEP 5: Train all anomaly detection models
      - name: Step 5 - Train anomaly detection models
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          echo "ğŸ¤– STEP 5: Training anomaly detection models..."
          echo ""
          echo "Models to train:"
          echo "  - Isolation Forest"
          echo "  - Local Outlier Factor (LOF)"
          echo "  - One-Class SVM"
          echo "  - DBSCAN (if enabled)"
          echo ""
          
          # Run complete training pipeline
          python src/models/train_anomaly_detection.py
          
          echo ""
          echo "âœ… Model training completed"
        env:
          MLFLOW_TRACKING_URI: "file:./mlruns"
      
      - name: Check training results
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          echo "ğŸ“Š Checking Training Results..."
          echo ""
          
          # Check models
          if [ -d "models/anomaly_detection" ]; then
            echo "âœ… Models trained:"
            find models/anomaly_detection -name "*.pkl" -type f | while read model; do
              echo "  - $model ($(du -h "$model" | cut -f1))"
            done
            echo ""
          else
            echo "âš ï¸  No models directory found"
          fi
          
          # Check reports
          if [ -f "outputs/models/reports/comprehensive_training_report.txt" ]; then
            echo "âœ… Training report generated"
            echo ""
            echo "=== REPORT PREVIEW ==="
            head -n 60 outputs/models/reports/comprehensive_training_report.txt
            echo ""
          else
            echo "âš ï¸  Training report not found"
          fi
          
          # Check sensitivity analysis
          if [ -f "outputs/models/plots/sensitivity_analysis.png" ]; then
            echo "âœ… Sensitivity analysis plot created"
          fi
          
          # Check model comparison
          if [ -f "outputs/models/plots/model_comparison.png" ]; then
            echo "âœ… Model comparison plot created"
          fi
        continue-on-error: true
      
      # Upload all training outputs to GCS
      - name: Upload models and outputs to GCS
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          echo "ğŸ“¤ Uploading trained models and outputs to GCS..."
          echo ""
          
          # Upload trained models (keep original path: anomaly_detection)
          if [ -d "models/anomaly_detection" ]; then
            echo "Uploading models to gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/..."
            gsutil -m rsync -r models/anomaly_detection/ gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/
            echo "âœ… Models uploaded"
            echo ""
          fi
          
          # Upload analysis outputs (plots, reports, results)
          if [ -d "outputs/models" ]; then
            echo "Uploading outputs to gs://${{ env.GCP_BUCKET }}/outputs/model3/..."
            gsutil -m rsync -r outputs/models/ gs://${{ env.GCP_BUCKET }}/outputs/model3/
            echo "âœ… Outputs uploaded"
            echo ""
          fi
          
          # Upload MLflow logs
          if [ -d "mlruns" ]; then
            echo "Uploading MLflow logs..."
            gsutil -m rsync -r mlruns/ gs://${{ env.GCP_BUCKET }}/mlruns/model3/
            echo "âœ… MLflow logs uploaded"
            echo ""
          fi
          
          echo "=========================================="
          echo "âœ… All artifacts uploaded to GCS!"
          echo "=========================================="
          echo ""
          echo "GCS Locations:"
          echo "  Bucket:     gs://${{ env.GCP_BUCKET }}/"
          echo "  Models:     gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/"
          echo "  Outputs:    gs://${{ env.GCP_BUCKET }}/outputs/model3/"
          echo "  EDA:        gs://${{ env.GCP_BUCKET }}/outputs/eda/"
          echo "  Snorkel:    gs://${{ env.GCP_BUCKET }}/outputs/snorkel/"
          echo "  MLflow:     gs://${{ env.GCP_BUCKET }}/mlruns/model3/"
        continue-on-error: true
      
      - name: Upload training artifacts to GitHub
        if: steps.check_data.outputs.data_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: anomaly-detection-models
          path: |
            models/anomaly_detection/**/*.pkl
            outputs/models/reports/*.txt
            outputs/models/plots/*.png
            outputs/models/results/*.csv
          retention-days: 30
        continue-on-error: true

  # Validate models and check deployment readiness
  validate-models:
    runs-on: ubuntu-latest
    needs: [train-anomaly-models]
    if: always()
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Download training artifacts
        uses: actions/download-artifact@v4
        with:
          name: anomaly-detection-models
          path: artifacts
        continue-on-error: true
      
      - name: Validate model performance
        run: |
          echo "ğŸ” Validating Model Performance..."
          echo ""
          
          if [ -f "artifacts/outputs/models/reports/comprehensive_training_report.txt" ]; then
            echo "âœ… Training report found"
            echo ""
            
            # Extract best model info
            echo "=== BEST MODEL SELECTED ==="
            grep -A 10 "BEST MODEL SELECTED" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
            echo ""
            
            # Extract validation results
            echo "=== VALIDATION STATUS ==="
            grep -A 15 "VALIDATION RESULTS" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
            echo ""
            
            # Extract bias detection results
            echo "=== BIAS DETECTION ==="
            grep -A 8 "BIAS DETECTION SUMMARY" artifacts/outputs/models/reports/comprehensive_training_report.txt || echo "Section not found"
            echo ""
            
          else
            echo "âš ï¸  Training report not found in artifacts"
          fi
        continue-on-error: true
      
      - name: Check deployment readiness in GCS
        run: |
          echo ""
          echo "ğŸš€ Checking Deployment Readiness in GCS..."
          echo ""
          
          # Check for models
          echo "Checking for trained models..."
          if gsutil ls gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/ > /dev/null 2>&1; then
            echo "âœ… Model directory exists in GCS"
            
            # Count model files
            MODEL_COUNT=$(gsutil ls gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/**/*.pkl 2>/dev/null | wc -l)
            
            if [ "$MODEL_COUNT" -gt 0 ]; then
              echo "âœ… Found $MODEL_COUNT model files"
              echo ""
              echo "Models in GCS:"
              gsutil ls gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/*/model.pkl 2>/dev/null | sed 's|gs://mlops-financial-stress-data/models/anomaly_detection/||' | sed 's|/model.pkl||' || gsutil ls gs://${{ env.GCP_BUCKET }}/models/anomaly_detection/*.pkl 2>/dev/null | sed 's|gs://mlops-financial-stress-data/models/anomaly_detection/||'
              echo ""
              echo "âœ… Models are READY for deployment"
            else
              echo "âŒ No model files found"
              echo "âŒ Models are NOT ready for deployment"
            fi
          else
            echo "âŒ Model directory not found in GCS"
            echo "âŒ Models are NOT ready for deployment"
          fi
          
          # Check for outputs
          echo ""
          echo "Checking for analysis outputs..."
          if gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/ > /dev/null 2>&1; then
            echo "âœ… Analysis outputs directory exists"
            
            # List key outputs
            echo ""
            echo "Available outputs:"
            gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.png 2>/dev/null | wc -l | xargs echo "  Plots:"
            gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.csv 2>/dev/null | wc -l | xargs echo "  Results:"
            gsutil ls gs://${{ env.GCP_BUCKET }}/outputs/model3/**/*.txt 2>/dev/null | wc -l | xargs echo "  Reports:"
          fi
        continue-on-error: true

  # Pipeline summary
  pipeline-summary:
    runs-on: ubuntu-latest
    needs: [run-tests, run-data-pipeline, train-anomaly-models, validate-models]
    if: always()
    
    steps:
      - name: Display comprehensive pipeline summary
        run: |
          echo "=========================================="
          echo "ANOMALY DETECTION CD PIPELINE SUMMARY"
          echo "=========================================="
          echo ""
          echo "Pipeline Execution Status:"
          echo "  Tests:              ${{ needs.run-tests.result }}"
          echo "  Data Pipeline:      ${{ needs.run-data-pipeline.result }}"
          echo "  Model Training:     ${{ needs.train-anomaly-models.result }}"
          echo "  Model Validation:   ${{ needs.validate-models.result }}"
          echo ""
          
          if [ "${{ needs.train-anomaly-models.result }}" == "success" ]; then
            echo "âœ… =========================================="
            echo "âœ… PIPELINE COMPLETED SUCCESSFULLY!"
            echo "âœ… =========================================="
            echo ""
            echo "ğŸ“Š Complete Pipeline Executed:"
            echo ""
            echo "  Step 1: âœ… Downloaded features_engineered.csv from GCS"
            echo "  Step 2: âœ… Ran EDA analysis (outputs/eda/)"
            echo "  Step 3: âœ… Extracted auto thresholds (outputs/snorkel/data/)"
            echo "  Step 4: âœ… Generated Snorkel labels (outputs/snorkel/data/)"
            echo "  Step 5: âœ… Trained anomaly detection models"
            echo ""
            echo "ğŸ¯ Models Trained:"
            echo "  âœ“ Isolation Forest"
            echo "  âœ“ Local Outlier Factor (LOF)"
            echo "  âœ“ One-Class SVM"
            echo "  âœ“ DBSCAN (if enabled)"
            echo ""
            echo "ğŸ“ˆ Analysis Completed:"
            echo "  âœ“ Hyperparameter sensitivity analysis"
            echo "  âœ“ Bias detection (sector slicing)"
            echo "  âœ“ SHAP feature importance"
            echo "  âœ“ Model comparison and selection"
            echo ""
            echo "ğŸ“¦ All Outputs Uploaded to GCS:"
            echo ""
            echo "  gs://mlops-financial-stress-data/"
            echo "  â”œâ”€â”€ data/"
            echo "  â”‚   â””â”€â”€ processed/features_engineered.csv"
            echo "  â”œâ”€â”€ models/"
            echo "  â”‚   â””â”€â”€ anomaly_detection/            â† Trained models"
            echo "  â”‚       â”œâ”€â”€ Isolation_Forest/"
            echo "  â”‚       â”œâ”€â”€ LOF/"
            echo "  â”‚       â”œâ”€â”€ One_Class_SVM/"
            echo "  â”‚       â””â”€â”€ DBSCAN/"
            echo "  â”œâ”€â”€ outputs/"
            echo "  â”‚   â”œâ”€â”€ eda/                           â† EDA analysis"
            echo "  â”‚   â”œâ”€â”€ snorkel/                       â† Labels & thresholds"
            echo "  â”‚   â””â”€â”€ model3/                        â† Training results"
            echo "  â”‚       â”œâ”€â”€ plots/                     â† Visualizations"
            echo "  â”‚       â”œâ”€â”€ results/                   â† Performance metrics"
            echo "  â”‚       â””â”€â”€ reports/                   â† Summary reports"
            echo "  â””â”€â”€ mlruns/"
            echo "      â””â”€â”€ model3/                        â† MLflow logs"
            echo ""
            echo "ğŸ”— Quick Access Links:"
            echo "  View all models:  gsutil ls gs://mlops-financial-stress-data/models/anomaly_detection/"
            echo "  View reports:     gsutil cat gs://mlops-financial-stress-data/outputs/model3/reports/comprehensive_training_report.txt"
            echo "  Download plots:   gsutil -m cp -r gs://mlops-financial-stress-data/outputs/model3/plots/ ./local-plots/"
            echo ""
            echo "=========================================="
            echo "âœ… READY FOR DEPLOYMENT!"
            echo "=========================================="
          else
            echo "âš ï¸  =========================================="
            echo "âš ï¸  PIPELINE COMPLETED WITH ISSUES"
            echo "âš ï¸  =========================================="
            echo ""
            echo "Check individual job logs for details:"
            echo "  - Data Pipeline: ${{ needs.run-data-pipeline.result }}"
            echo "  - Model Training: ${{ needs.train-anomaly-models.result }}"
            echo "  - Validation: ${{ needs.validate-models.result }}"
            echo ""
            echo "Review the workflow run for error messages"
          fi
          echo ""
      
      - name: Set status annotations
        run: |
          if [ "${{ needs.train-anomaly-models.result }}" == "success" ]; then
            echo "::notice title=Pipeline Complete::Anomaly detection pipeline executed successfully"
            echo "::notice title=Models Ready::Models available at gs://mlops-financial-stress-data/models/model3/"
            echo "::notice title=Full Pipeline::EDA â†’ Thresholds â†’ Snorkel â†’ Training completed"
          else
            echo "::warning title=Pipeline Issues::Check individual job logs for details"
          fi