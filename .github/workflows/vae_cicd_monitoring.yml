name: VAE Model - CI/CD with Drift Detection & Retraining

on:
  # Trigger on code changes
  push:
    branches:
      - main
    paths:
      - "src/models/vae/**"
      - "tests/**"
      - ".github/workflows/vae_cicd_monitoring.yml"
  
  # Scheduled drift detection (runs daily at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force VAE retraining'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

env:
  PYTHON_VERSION: "3.10"
  GCP_BUCKET: "mlops-financial-stress-data"
  GCP_PROJECT: "ninth-iris-422916-f2"
  KS_THRESHOLD: "0.70"  # Your project's KS statistic threshold
  MLFLOW_TRACKING_URI: "file:./mlruns"  # Local MLflow for GitHub Actions

jobs:
  # ============================================
  # JOB 1: Run Tests
  # ============================================
  run-tests:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run VAE tests
        run: |
          if [ -d "tests" ]; then
            pytest tests/ -v || echo "Tests completed with warnings"
          else
            echo "‚ö†Ô∏è  No tests directory found"
          fi
        continue-on-error: true

  # ============================================
  # JOB 2: Drift Detection for VAE Models
  # ============================================
  detect-drift:
    runs-on: ubuntu-latest
    needs: [run-tests]
    outputs:
      drift_detected: ${{ steps.drift_check.outputs.drift_detected }}
      avg_ks_statistic: ${{ steps.drift_check.outputs.avg_ks_statistic }}
      should_retrain: ${{ steps.drift_check.outputs.should_retrain }}
      drift_reason: ${{ steps.drift_check.outputs.drift_reason }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install google-cloud-storage scipy numpy pandas scikit-learn

      - name: Download reference data and current model
        run: |
          echo "üì• Downloading data from GCS..."
          mkdir -p data/reference models/current
          
          # Download training data (reference distribution)
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/features/macro_features.csv data/reference/train_data.csv || echo "‚ö†Ô∏è  Training data not found"
          
          # Download current production model
          gsutil cp gs://${{ env.GCP_BUCKET }}/models/vae/dense_vae_best.h5 models/current/ || echo "‚ö†Ô∏è  Current model not found"
          
          # Download model metrics
          gsutil cp gs://${{ env.GCP_BUCKET }}/models/vae/vae_metrics.json models/current/ || echo "‚ö†Ô∏è  Metrics not found"
          
          # Download recent production data (if logging predictions)
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/production/recent_scenarios.csv data/reference/ || echo "‚ö†Ô∏è  No production data, will use validation data"
          
          echo "‚úÖ Download complete"

      - name: Run KS Statistic Drift Detection
        id: drift_check
        run: |
          python - <<'EOF'
          import json
          import numpy as np
          import pandas as pd
          from scipy import stats
          import os
          import sys
          from datetime import datetime
          
          print("="*70)
          print("üîç VAE MODEL DRIFT DETECTION")
          print("="*70)
          print(f"Timestamp: {datetime.now().isoformat()}")
          print(f"KS Threshold: {os.getenv('KS_THRESHOLD', '0.70')}")
          print("")
          
          # Load reference data (training data distribution)
          try:
              train_data = pd.read_csv('data/reference/train_data.csv')
              print(f"‚úÖ Loaded training data: {train_data.shape}")
          except Exception as e:
              print(f"‚ùå Could not load training data: {e}")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("drift_detected=false\n")
                  f.write("should_retrain=false\n")
                  f.write("avg_ks_statistic=0.0\n")
                  f.write("drift_reason=No training data available\n")
              sys.exit(0)
          
          # Load production/validation data
          try:
              prod_data = pd.read_csv('data/reference/recent_scenarios.csv')
              print(f"‚úÖ Loaded production data: {prod_data.shape}")
          except:
              print("‚ö†Ô∏è  No production data, simulating with training data subset")
              # Use 20% of training data as "production" for demo
              prod_data = train_data.sample(frac=0.2, random_state=42)
          
          # Load current model metrics
          try:
              with open('models/current/vae_metrics.json', 'r') as f:
                  current_metrics = json.load(f)
              print(f"‚úÖ Current model metrics:")
              print(f"   KS Statistic: {current_metrics.get('avg_ks_statistic', 'N/A')}")
              print(f"   Pass Rate: {current_metrics.get('ks_pass_rate', 'N/A')}")
          except Exception as e:
              print(f"‚ö†Ô∏è  No current metrics found: {e}")
              current_metrics = {}
          
          print("\n" + "="*70)
          print("üìä CALCULATING KS STATISTICS")
          print("="*70)
          
          # Select numeric columns (macroeconomic features)
          numeric_cols = train_data.select_dtypes(include=[np.number]).columns
          numeric_cols = [col for col in numeric_cols if col in prod_data.columns]
          
          print(f"Checking {len(numeric_cols)} features...")
          
          ks_statistics = []
          feature_drift = {}
          
          for col in numeric_cols:
              try:
                  train_values = train_data[col].dropna()
                  prod_values = prod_data[col].dropna()
                  
                  if len(train_values) > 10 and len(prod_values) > 10:
                      ks_stat, p_value = stats.ks_2samp(train_values, prod_values)
                      ks_statistics.append(ks_stat)
                      feature_drift[col] = {
                          'ks_statistic': float(ks_stat),
                          'p_value': float(p_value),
                          'status': 'PASS' if ks_stat >= float(os.getenv('KS_THRESHOLD', '0.70')) else 'FAIL'
                      }
                      
                      status_icon = "‚úÖ" if ks_stat >= 0.70 else "‚ö†Ô∏è "
                      print(f"   {status_icon} {col:30s} KS={ks_stat:.4f}, p={p_value:.4f}")
              except Exception as e:
                  print(f"   ‚ùå {col:30s} Error: {e}")
          
          # Calculate average KS statistic
          avg_ks_stat = np.mean(ks_statistics) if ks_statistics else 0.0
          ks_threshold = float(os.getenv('KS_THRESHOLD', '0.70'))
          pass_rate = sum(1 for ks in ks_statistics if ks >= ks_threshold) / len(ks_statistics) if ks_statistics else 0
          
          print("\n" + "="*70)
          print("üìä DRIFT DETECTION RESULTS")
          print("="*70)
          print(f"Average KS Statistic: {avg_ks_stat:.4f}")
          print(f"Threshold: {ks_threshold}")
          print(f"Pass Rate: {pass_rate*100:.1f}% ({sum(1 for ks in ks_statistics if ks >= ks_threshold)}/{len(ks_statistics)} features)")
          
          # Determine if drift detected
          ks_drift = avg_ks_stat < ks_threshold
          low_pass_rate = pass_rate < 0.70  # Less than 70% features pass
          
          drift_reasons = []
          if ks_drift:
              drift_reasons.append(f"Average KS statistic ({avg_ks_stat:.4f}) below threshold ({ks_threshold})")
          if low_pass_rate:
              drift_reasons.append(f"Low pass rate ({pass_rate*100:.1f}% < 70%)")
          
          drift_detected = ks_drift or low_pass_rate
          force_retrain = os.getenv('INPUT_FORCE_RETRAIN', 'false') == 'true'
          should_retrain = drift_detected or force_retrain
          
          print("\n" + "="*70)
          print("üéØ FINAL DECISION")
          print("="*70)
          
          if force_retrain:
              print("üîÑ FORCE RETRAIN TRIGGERED (Manual)")
              drift_reasons.append("Manual force retrain requested")
          elif drift_detected:
              print("‚ö†Ô∏è  DRIFT DETECTED - RETRAINING REQUIRED")
              print("\nReasons:")
              for i, reason in enumerate(drift_reasons, 1):
                  print(f"   {i}. {reason}")
          else:
              print("‚úÖ NO DRIFT DETECTED - Model is healthy")
          
          # Save drift report
          drift_report = {
              "timestamp": datetime.now().isoformat(),
              "drift_detected": drift_detected,
              "should_retrain": should_retrain,
              "metrics": {
                  "avg_ks_statistic": float(avg_ks_stat),
                  "ks_threshold": ks_threshold,
                  "pass_rate": float(pass_rate),
                  "num_features_checked": len(ks_statistics)
              },
              "feature_drift": feature_drift,
              "drift_reasons": drift_reasons,
              "force_retrain": force_retrain
          }
          
          os.makedirs('reports', exist_ok=True)
          with open('reports/drift_report.json', 'w') as f:
              json.dump(drift_report, f, indent=2)
          
          print("\n‚úÖ Drift report saved to reports/drift_report.json")
          
          # Write GitHub Actions outputs
          drift_reason_str = " | ".join(drift_reasons) if drift_reasons else "No drift detected"
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"drift_detected={str(drift_detected).lower()}\n")
              f.write(f"avg_ks_statistic={avg_ks_stat:.4f}\n")
              f.write(f"should_retrain={str(should_retrain).lower()}\n")
              f.write(f"drift_reason={drift_reason_str}\n")
          
          print("\n" + "="*70)
          EOF

      - name: Upload drift report to GCS
        if: always()
        run: |
          timestamp=$(date +%Y%m%d_%H%M%S)
          gsutil cp reports/drift_report.json gs://${{ env.GCP_BUCKET }}/monitoring/drift_reports/vae_drift_${timestamp}.json
          echo "‚úÖ Drift report uploaded to GCS"

      - name: Upload drift report as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: drift-report
          path: reports/drift_report.json
          retention-days: 30

  # ============================================
  # JOB 3: Retrain VAE (Conditional - only if drift detected)
  # ============================================
  retrain-vae:
    runs-on: ubuntu-latest
    needs: [detect-drift]
    if: needs.detect-drift.outputs.should_retrain == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          df -h

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install mlflow

      - name: Pull latest data from GCS
        run: |
          echo "üì• Downloading latest training data..."
          mkdir -p data/features data/processed
          
          # Download latest macro features
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/features/macro_features.csv data/features/ || \
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/processed/features_engineered.csv data/features/macro_features.csv
          
          echo "‚úÖ Latest data downloaded"
          ls -lh data/features/

      - name: Setup MLflow
        run: |
          mkdir -p mlruns
          export MLFLOW_TRACKING_URI=file:./mlruns
          echo "‚úÖ MLflow tracking URI: $MLFLOW_TRACKING_URI"

      - name: Train Dense VAE
        run: |
          echo "üöÄ Training Dense VAE..."
          python src/models/vae/Dense_VAE_optimized_mlflow_updated.py \
            --data data/features/macro_features.csv \
            --epochs 50 \
            --batch-size 32 \
            --latent-dim 64 \
            --output models/vae
          echo "‚úÖ Dense VAE training complete"
        continue-on-error: true

      - name: Train Ensemble VAE
        run: |
          echo "üöÄ Training Ensemble VAE..."
          python src/models/vae/Ensemble_VAE_updated.py \
            --data data/features/macro_features.csv \
            --epochs 50 \
            --output models/vae
          echo "‚úÖ Ensemble VAE training complete"
        continue-on-error: true

      - name: Select best VAE model
        id: select_model
        run: |
          python - <<'EOF'
          import json
          import os
          import glob
          
          print("="*70)
          print("üèÜ SELECTING BEST VAE MODEL")
          print("="*70)
          
          # Find all VAE model metrics
          metrics_files = glob.glob('models/vae/*_metrics.json')
          
          if not metrics_files:
              print("‚ùå No model metrics found!")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("model_selected=false\n")
              exit(1)
          
          print(f"Found {len(metrics_files)} model(s)")
          
          best_model = None
          best_ks_stat = 0.0
          best_metrics = None
          
          for metrics_file in metrics_files:
              try:
                  with open(metrics_file, 'r') as f:
                      metrics = json.load(f)
                  
                  model_name = os.path.basename(metrics_file).replace('_metrics.json', '')
                  avg_ks = metrics.get('avg_ks_statistic', 0.0)
                  pass_rate = metrics.get('ks_pass_rate', 0.0)
                  
                  print(f"\nüìä {model_name}:")
                  print(f"   KS Statistic: {avg_ks:.4f}")
                  print(f"   Pass Rate: {pass_rate*100:.1f}%")
                  
                  if avg_ks > best_ks_stat:
                      best_ks_stat = avg_ks
                      best_model = model_name
                      best_metrics = metrics
              except Exception as e:
                  print(f"‚ö†Ô∏è  Error reading {metrics_file}: {e}")
          
          if best_model:
              print("\n" + "="*70)
              print(f"üèÜ BEST MODEL: {best_model}")
              print(f"   KS Statistic: {best_ks_stat:.4f}")
              print(f"   Threshold: {os.getenv('KS_THRESHOLD', '0.70')}")
              
              # Check if meets threshold
              meets_threshold = best_ks_stat >= float(os.getenv('KS_THRESHOLD', '0.70'))
              
              if meets_threshold:
                  print(f"‚úÖ Model PASSES threshold!")
              else:
                  print(f"‚ö†Ô∏è  Model below threshold but is best available")
              
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"model_selected=true\n")
                  f.write(f"best_model_name={best_model}\n")
                  f.write(f"best_ks_statistic={best_ks_stat:.4f}\n")
                  f.write(f"meets_threshold={str(meets_threshold).lower()}\n")
          else:
              print("‚ùå No valid model found")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("model_selected=false\n")
          
          print("="*70)
          EOF

      - name: Compare with production model
        id: compare
        if: steps.select_model.outputs.model_selected == 'true'
        run: |
          python - <<'EOF'
          import json
          import os
          
          print("="*70)
          print("üìä COMPARING NEW MODEL WITH PRODUCTION")
          print("="*70)
          
          new_ks = float(os.getenv('BEST_KS_STATISTIC', '0.0'))
          print(f"New Model KS: {new_ks:.4f}")
          
          # Download current production metrics
          os.system('gsutil cp gs://mlops-financial-stress-data/models/vae/vae_metrics.json models/prod_metrics.json 2>/dev/null || echo "{}" > models/prod_metrics.json')
          
          try:
              with open('models/prod_metrics.json', 'r') as f:
                  content = f.read()
                  if content.strip():
                      prod_metrics = json.loads(content)
                  else:
                      prod_metrics = {}
          except:
              prod_metrics = {}
          
          prod_ks = prod_metrics.get('avg_ks_statistic', 0.0)
          print(f"Production Model KS: {prod_ks:.4f}")
          
          # Decision logic
          deploy = False
          reason = ""
          
          if prod_ks == 0.0:
              # No production model exists
              deploy = True
              reason = "No existing production model"
          elif new_ks > prod_ks:
              # New model is better
              improvement = ((new_ks - prod_ks) / prod_ks * 100) if prod_ks > 0 else 100
              deploy = True
              reason = f"New model improves KS by {improvement:.1f}%"
          elif new_ks >= float(os.getenv('KS_THRESHOLD', '0.70')):
              # New model meets threshold even if not better
              deploy = True
              reason = "New model meets threshold requirement"
          else:
              reason = "New model does not improve performance"
          
          print("\n" + "="*70)
          print("üéØ DEPLOYMENT DECISION")
          print("="*70)
          
          if deploy:
              print(f"‚úÖ DEPLOY NEW MODEL")
              print(f"   Reason: {reason}")
          else:
              print(f"‚ùå KEEP PRODUCTION MODEL")
              print(f"   Reason: {reason}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"deploy_model={str(deploy).lower()}\n")
              f.write(f"deployment_reason={reason}\n")
          
          print("="*70)
          EOF
        env:
          BEST_KS_STATISTIC: ${{ steps.select_model.outputs.best_ks_statistic }}

      - name: Register model in MLflow Model Registry
        if: steps.compare.outputs.deploy_model == 'true'
        run: |
          echo "üìù Registering model in MLflow Model Registry..."
          
          python - <<'EOF'
          import mlflow
          import os
          from datetime import datetime
          
          # Set MLflow tracking URI
          mlflow.set_tracking_uri("file:./mlruns")
          
          model_name = os.getenv('BEST_MODEL_NAME', 'dense_vae')
          ks_stat = os.getenv('BEST_KS_STATISTIC', '0.0')
          
          print(f"Registering model: {model_name}")
          print(f"KS Statistic: {ks_stat}")
          
          # Find the latest run
          client = mlflow.tracking.MlflowClient()
          experiment = client.get_experiment_by_name("VAE_Training")
          
          if experiment:
              runs = client.search_runs(
                  experiment_ids=[experiment.experiment_id],
                  order_by=["start_time DESC"],
                  max_results=1
              )
              
              if runs:
                  run_id = runs[0].info.run_id
                  print(f"Found run: {run_id}")
                  
                  # Register model
                  model_uri = f"runs:/{run_id}/model"
                  
                  try:
                      mv = mlflow.register_model(
                          model_uri=model_uri,
                          name="financial_stress_vae"
                      )
                      
                      print(f"‚úÖ Model registered:")
                      print(f"   Name: {mv.name}")
                      print(f"   Version: {mv.version}")
                      
                      # Transition to Production
                      client.transition_model_version_stage(
                          name="financial_stress_vae",
                          version=mv.version,
                          stage="Production"
                      )
                      
                      print(f"‚úÖ Model transitioned to Production stage")
                      
                      # Save version info
                      version_info = {
                          "model_name": mv.name,
                          "version": mv.version,
                          "run_id": run_id,
                          "ks_statistic": float(ks_stat),
                          "registered_at": datetime.now().isoformat()
                      }
                      
                      import json
                      with open('models/vae/model_version.json', 'w') as f:
                          json.dump(version_info, f, indent=2)
                      
                  except Exception as e:
                      print(f"‚ö†Ô∏è  Model registration note: {e}")
                      print("   (This is normal if model already exists)")
              else:
                  print("‚ö†Ô∏è  No runs found")
          else:
              print("‚ö†Ô∏è  Experiment not found")
          EOF
        env:
          BEST_MODEL_NAME: ${{ steps.select_model.outputs.best_model_name }}
          BEST_KS_STATISTIC: ${{ steps.select_model.outputs.best_ks_statistic }}

      - name: Deploy to GCS (Production)
        if: steps.compare.outputs.deploy_model == 'true'
        run: |
          echo "üöÄ Deploying new model to production..."
          
          timestamp=$(date +%Y%m%d_%H%M%S)
          best_model="${{ steps.select_model.outputs.best_model_name }}"
          
          # Backup current production model
          echo "üì¶ Backing up current production model..."
          gsutil cp gs://${{ env.GCP_BUCKET }}/models/vae/dense_vae_best.h5 \
                     gs://${{ env.GCP_BUCKET }}/models/vae/backups/dense_vae_${timestamp}.h5 2>/dev/null || \
                     echo "No existing model to backup"
          
          # Deploy new model
          echo "üì§ Uploading new model..."
          gsutil -m cp models/vae/${best_model}.h5 gs://${{ env.GCP_BUCKET }}/models/vae/dense_vae_best.h5
          gsutil cp models/vae/${best_model}_metrics.json gs://${{ env.GCP_BUCKET }}/models/vae/vae_metrics.json
          
          # Upload model version info
          if [ -f "models/vae/model_version.json" ]; then
            gsutil cp models/vae/model_version.json gs://${{ env.GCP_BUCKET }}/models/vae/
          fi
          
          # Upload MLflow artifacts
          echo "üì§ Uploading MLflow artifacts..."
          gsutil -m rsync -r mlruns/ gs://${{ env.GCP_BUCKET }}/mlflow/mlruns/
          
          echo "‚úÖ Model deployed to production!"
          echo "   Model: ${best_model}"
          echo "   KS Statistic: ${{ steps.select_model.outputs.best_ks_statistic }}"
          echo "   Timestamp: ${timestamp}"

      - name: Upload training artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: vae-training-artifacts
          path: |
            models/vae/*.h5
            models/vae/*_metrics.json
            mlruns/
          retention-days: 30

  # ============================================
  # JOB 4: Pipeline Summary
  # ============================================
  pipeline-summary:
    runs-on: ubuntu-latest
    needs: [run-tests, detect-drift, retrain-vae]
    if: always()
    
    steps:
      - name: Display Pipeline Summary
        run: |
          echo "=========================================="
          echo "   VAE CI/CD PIPELINE SUMMARY"
          echo "=========================================="
          echo ""
          echo "üìã Job Results:"
          echo "  Tests: ${{ needs.run-tests.result }}"
          echo "  Drift Detection: ${{ needs.detect-drift.result }}"
          echo "  Model Retraining: ${{ needs.retrain-vae.result }}"
          echo ""
          echo "üìä Drift Detection:"
          echo "  Drift Detected: ${{ needs.detect-drift.outputs.drift_detected }}"
          echo "  Should Retrain: ${{ needs.detect-drift.outputs.should_retrain }}"
          echo "  Avg KS Statistic: ${{ needs.detect-drift.outputs.avg_ks_statistic }}"
          echo "  Reason: ${{ needs.detect-drift.outputs.drift_reason }}"
          echo ""
          
          if [ "${{ needs.retrain-vae.result }}" == "success" ]; then
            echo "‚úÖ NEW MODEL DEPLOYED TO PRODUCTION"
            echo ""
            echo "üéâ Pipeline completed successfully!"
          elif [ "${{ needs.detect-drift.outputs.should_retrain }}" == "true" ]; then
            echo "‚ö†Ô∏è  Retraining was triggered but may have encountered issues"
            echo "   Check logs for details"
          else
            echo "‚ÑπÔ∏è  NO RETRAINING NEEDED"
            echo "   Current model is performing well"
          fi
          echo ""
          echo "=========================================="
