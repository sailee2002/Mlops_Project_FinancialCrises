"""
STEP 4: ANOMALY DETECTION (FLAG ONLY - NO DATA MODIFICATION) - WITH GCS

MODIFIED FOR QUARTERLY DATA:
- More lenient outlier detection (5 IQR vs 3 IQR)
- Higher jump threshold for quarterly volatility (100% vs 50%)
- Accept higher percentages of anomalies
- Proper JSON serialization
- GCS integration for input/output

Detects anomalies and adds flag columns for model awareness.

Anomaly Types Detected:
1. Statistical Outliers (IQR method)
2. Business Rule Violations (negative prices, impossible ratios)
3. Temporal Anomalies (sudden jumps)

Usage:
    python step6_anomaly_detection.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple
from datetime import datetime
import json
import warnings
from google.cloud import storage
import io

warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================================================
# JSON SERIALIZATION FIX
# ============================================================================

class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy/pandas types."""
    
    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif pd.isna(obj):
            return None
        return super().default(obj)


# ============================================================================
# ANOMALY DETECTOR WITH GCS
# ============================================================================

class AnomalyDetectorQuarterly:
    """
    Anomaly detector for quarterly data with GCS integration.
    
    MODIFIED: More lenient thresholds for quarterly volatility + GCS support.
    """
    
    def __init__(self, dataset_name: str, bucket_name: str = 'mlops-financial-stress-data', use_gcs: bool = True):
        self.dataset_name = dataset_name
        self.bucket_name = bucket_name
        self.use_gcs = use_gcs
        
        self.anomaly_report = {
            'dataset': dataset_name,
            'timestamp': datetime.now().isoformat(),
            'anomalies': [],
            'critical_count': 0,
            'total_count': 0,
            'flags_created': []
        }
        
        # Initialize GCS client
        if self.use_gcs:
            try:
                self.storage_client = storage.Client()
                self.bucket = self.storage_client.bucket(bucket_name)
                logger.info(f"‚úì Connected to GCS bucket: gs://{bucket_name}/")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  GCS connection failed: {e}")
                logger.warning("  Will use local files only")
                self.use_gcs = False
    
    def upload_to_gcs(self, content: str, filename: str, subfolder: str = 'data/anomaly_reports'):
        """Upload text content to GCS."""
        if not self.use_gcs:
            return False
        
        try:
            blob_path = f"{subfolder}/{filename}"
            blob = self.bucket.blob(blob_path)
            blob.upload_from_string(content, content_type='application/json')
            
            logger.info(f"  ‚úì Uploaded to GCS: gs://{self.bucket_name}/{blob_path}")
            return True
            
        except Exception as e:
            logger.error(f"  ‚úó GCS upload failed: {e}")
            return False
    
    def upload_csv_to_gcs(self, df: pd.DataFrame, filename: str, subfolder: str = 'data/processed'):
        """Upload DataFrame to GCS as CSV."""
        if not self.use_gcs:
            return False
        
        try:
            blob_path = f"{subfolder}/{filename}"
            blob = self.bucket.blob(blob_path)
            
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False)
            blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
            
            size_mb = len(csv_buffer.getvalue()) / (1024 * 1024)
            logger.info(f"  ‚úì Uploaded to GCS: gs://{self.bucket_name}/{blob_path} ({size_mb:.2f} MB)")
            return True
            
        except Exception as e:
            logger.error(f"  ‚úó GCS upload failed: {e}")
            return False
    
    # ========== METHOD 1: STATISTICAL OUTLIERS (IQR) ==========
    
    def detect_statistical_outliers(self, df: pd.DataFrame, group_col: str = None) -> pd.DataFrame:
        """
        Detect statistical outliers using IQR method.
        
        MODIFIED: Uses 5 IQR (was 3) for quarterly data volatility.
        """
        logger.info("\n[1/3] Statistical Outlier Detection (5 IQR for quarterly)...")
        
        df_flagged = df.copy()
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        exclude_cols = ['Date', 'Year', 'Month', 'Day', 'Quarter']
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        outlier_summary = []
        
        # Select important columns to flag
        important_cols = [
            'Stock_Price', 'Revenue', 'Net_Income', 'Total_Assets',
            'VIX', 'GDP', 'Unemployment_Rate'
        ]
        
        cols_to_flag = [col for col in important_cols if col in numeric_cols]
        
        logger.info(f"   Analyzing {len(numeric_cols)} numeric columns...")
        logger.info(f"   Will create flags for {len(cols_to_flag)} key columns...")
        
        for col in numeric_cols:
            if group_col and group_col in df.columns:
                # Detect per group
                for group in df[group_col].unique():
                    group_mask = df[group_col] == group
                    group_data = df.loc[group_mask, col].dropna()
                    
                    if len(group_data) < 10:
                        continue
                    
                    Q1 = group_data.quantile(0.25)
                    Q3 = group_data.quantile(0.75)
                    IQR = Q3 - Q1
                    
                    # MODIFIED: 5 IQR instead of 3 (more lenient for quarterly)
                    lower = Q1 - 5 * IQR
                    upper = Q3 + 5 * IQR
                    
                    outlier_mask_group = (group_data < lower) | (group_data > upper)
                    outlier_count = outlier_mask_group.sum()
                    
                    if outlier_count > 0:
                        outlier_summary.append({
                            'column': str(col),
                            'group': str(group),
                            'count': int(outlier_count),
                            'percentage': float(round((outlier_count / len(group_data)) * 100, 2))
                        })
                        
                        # Add flag column
                        if col in cols_to_flag:
                            flag_col = f"{col}_Outlier_Flag"
                            
                            if flag_col not in df_flagged.columns:
                                df_flagged[flag_col] = 0
                            
                            outlier_indices = group_data[outlier_mask_group].index
                            df_flagged.loc[outlier_indices, flag_col] = 1
            else:
                # Detect globally
                data = df[col].dropna()
                
                if len(data) < 10:
                    continue
                
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                
                lower = Q1 - 5 * IQR
                upper = Q3 + 5 * IQR
                
                outlier_mask = (df[col] < lower) | (df[col] > upper)
                outlier_count = outlier_mask.sum()
                
                if outlier_count > 0:
                    outlier_summary.append({
                        'column': str(col),
                        'count': int(outlier_count),
                        'percentage': float(round((outlier_count / len(data)) * 100, 2))
                    })
                    
                    if col in cols_to_flag:
                        flag_col = f"{col}_Outlier_Flag"
                        df_flagged[flag_col] = outlier_mask.astype(int)
        
        # Log results
        if outlier_summary:
            outlier_df = pd.DataFrame(outlier_summary)
            outlier_df = outlier_df.sort_values('count', ascending=False)
            
            logger.info(f"   Found outliers in {len(outlier_summary)} column-group combinations:")
            print(outlier_df.head(10).to_string(index=False))
            
            flags_created = [col for col in df_flagged.columns if col.endswith('_Outlier_Flag')]
            logger.info(f"\n   ‚úì Created {len(flags_created)} outlier flag columns")
            
            self.anomaly_report['statistical_outliers'] = outlier_summary
            self.anomaly_report['flags_created'].extend(flags_created)
            self.anomaly_report['total_count'] += sum(item['count'] for item in outlier_summary)
        else:
            logger.info(f"   ‚úì No statistical outliers detected")
        
        return df_flagged
    
    # ========== METHOD 2: BUSINESS RULE VIOLATIONS ==========
    
    def detect_business_rules(self, df: pd.DataFrame) -> pd.DataFrame:
        """Detect business rule violations."""
        logger.info("\n[2/3] Business Rule Violation Detection...")
        
        df_flagged = df.copy()
        violations = []
        
        # === RULE 1: Negative values where impossible ===
        non_negative_rules = {
            'VIX': 'VIX cannot be negative',
            'Volume': 'Trading volume cannot be negative',
            'CPI': 'CPI cannot be negative',
            'Total_Assets': 'Total assets cannot be negative',
            'Stock_Price': 'Stock price cannot be negative',
            'Close': 'Close price cannot be negative',
            'Revenue': 'Revenue cannot be negative'
        }
        
        for col, rule in non_negative_rules.items():
            if col in df.columns:
                negative_mask = df[col] < 0
                negative_count = negative_mask.sum()
                
                if negative_count > 0:
                    violations.append({
                        'rule': str(rule),
                        'column': str(col),
                        'count': int(negative_count),
                        'severity': 'CRITICAL'
                    })
                    
                    flag_col = f"{col}_Negative_Flag"
                    df_flagged[flag_col] = negative_mask.astype(int)
                    self.anomaly_report['flags_created'].append(flag_col)
                    
                    self.anomaly_report['critical_count'] += int(negative_count)
        
        # === RULE 2: Impossible ratios ===
        ratio_rules = {
            'Debt_to_Equity': (0, 100, 'Debt-to-equity typically < 100'),
            'Current_Ratio': (0, 50, 'Current ratio typically < 50'),
        }
        
        for col, (min_val, max_val, rule) in ratio_rules.items():
            if col in df.columns:
                violation_mask = (df[col] < min_val) | (df[col] > max_val)
                violation_count = violation_mask.sum()
                
                if violation_count > 0:
                    violations.append({
                        'rule': str(rule),
                        'column': str(col),
                        'count': int(violation_count),
                        'severity': 'HIGH',
                        'range': [float(min_val), float(max_val)]
                    })
                    
                    flag_col = f"{col}_Extreme_Flag"
                    df_flagged[flag_col] = violation_mask.astype(int)
                    self.anomaly_report['flags_created'].append(flag_col)
        
        # Log violations
        if violations:
            logger.info(f"   Found {len(violations)} business rule violations:")
            
            for v in violations:
                severity_icon = "üö®" if v['severity'] == 'CRITICAL' else "‚ö†Ô∏è"
                logger.info(f"   {severity_icon} {v['rule']}")
                logger.info(f"      Column: {v['column']}, Count: {v['count']}")
            
            self.anomaly_report['business_rule_violations'] = violations
            self.anomaly_report['total_count'] += sum(v['count'] for v in violations)
        else:
            logger.info(f"   ‚úì No business rule violations")
        
        return df_flagged
    
    # ========== METHOD 3: TEMPORAL ANOMALIES ==========
    
    def detect_temporal_anomalies(self, df: pd.DataFrame, group_col: str = None) -> pd.DataFrame:
        """
        Detect temporal anomalies (sudden jumps).
        
        MODIFIED: 100% jump threshold (was 50%) for quarterly volatility.
        """
        logger.info("\n[3/3] Temporal Anomaly Detection (100% threshold for quarterly)...")
        
        if 'Date' not in df.columns:
            logger.info("   No Date column - skipping temporal detection")
            return df
        
        df_flagged = df.copy()
        temporal_anomalies = []
        
        # Check key columns
        check_cols = ['Stock_Price', 'Close', 'Revenue']
        available_cols = [col for col in check_cols if col in df.columns]
        
        if not available_cols:
            logger.info("   No suitable columns for temporal detection")
            return df_flagged
        
        logger.info(f"   Checking {len(available_cols)} columns for sudden jumps...")
        
        for col in available_cols:
            flag_col = f"{col}_Jump_Flag"
            df_flagged[flag_col] = 0
            
            if group_col and group_col in df.columns:
                for company in df[group_col].unique():
                    company_df = df[df[group_col] == company].sort_values('Date').copy()
                    
                    if len(company_df) < 10:
                        continue
                    
                    pct_change = company_df[col].pct_change().abs() * 100
                    
                    # MODIFIED: 100% jump threshold (was 50%) for quarterly
                    jump_mask = pct_change > 100
                    jump_count = jump_mask.sum()
                    
                    if jump_count > 0:
                        jump_indices = company_df[jump_mask].index
                        df_flagged.loc[jump_indices, flag_col] = 1
                        
                        jump_years = [int(year) for year in company_df.loc[jump_mask, 'Date'].dt.year.unique()]
                        crisis_years = [1990, 2000, 2001, 2008, 2009, 2020]
                        is_crisis = any(year in crisis_years for year in jump_years)
                        
                        temporal_anomalies.append({
                            'column': str(col),
                            'company': str(company),
                            'count': int(jump_count),
                            'years': jump_years,
                            'is_crisis': bool(is_crisis),
                            'severity': 'LOW' if is_crisis else 'HIGH'
                        })
                        
                        if not is_crisis:
                            self.anomaly_report['critical_count'] += int(jump_count)
        
        if temporal_anomalies:
            logger.info(f"   Found {len(temporal_anomalies)} temporal anomaly patterns:")
            
            for anomaly in temporal_anomalies[:10]:
                severity_icon = "üìä" if anomaly.get('is_crisis', False) else "‚ö†Ô∏è"
                company_str = f" ({anomaly['company']})" if 'company' in anomaly else ""
                logger.info(f"   {severity_icon} {anomaly['column']}{company_str}: " +
                          f"{anomaly['count']} jumps in years {anomaly['years']}")
            
            flags_created = [col for col in df_flagged.columns if col.endswith('_Jump_Flag')]
            logger.info(f"\n   ‚úì Created {len(flags_created)} jump flag columns")
            
            self.anomaly_report['temporal_anomalies'] = temporal_anomalies
            self.anomaly_report['flags_created'].extend(flags_created)
        else:
            logger.info(f"   ‚úì No temporal anomalies detected")
        
        return df_flagged
    
    # ========== ALERTING ==========
    
    def send_alert(self):
        """Send alert if critical anomalies found."""
        
        if self.anomaly_report['critical_count'] == 0:
            logger.info("\n‚úì No critical anomalies - no alert needed")
            return
        
        logger.warning(f"\nüö® CRITICAL ANOMALIES DETECTED!")
        
        alert_message = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           CRITICAL ANOMALIES DETECTED                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Dataset: {self.dataset_name}
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Critical Anomalies: {self.anomaly_report['critical_count']:,}
Total Anomalies: {self.anomaly_report['total_count']:,}

ACTION REQUIRED:
1. Review anomaly report in data/anomaly_reports/
2. Verify data sources for critical issues
3. Check if anomalies are valid (crisis periods) or errors

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """
        
        logger.warning(alert_message)
    
    # ========== MAIN PIPELINE ==========
    
    def run_detection(self, df: pd.DataFrame, group_col: str = None) -> Tuple[pd.DataFrame, Dict]:
        """Run all anomaly detection methods."""
        
        logger.info("\n" + "="*80)
        logger.info("ANOMALY DETECTION (QUARTERLY-AWARE) WITH GCS")
        logger.info("="*80)
        logger.info(f"Dataset: {self.dataset_name}")
        logger.info(f"Shape: {df.shape}")
        logger.info(f"Mode: FLAG ONLY - No data modification")
        logger.info(f"Thresholds: 5 IQR, 100% jump (lenient for quarterly)")
        logger.info(f"GCS Enabled: {self.use_gcs}")
        logger.info("="*80)
        
        # Run all detections
        df_result = self.detect_statistical_outliers(df, group_col)
        df_result = self.detect_business_rules(df_result)
        df_result = self.detect_temporal_anomalies(df_result, group_col)
        
        # Summary
        logger.info("\n" + "="*80)
        logger.info("ANOMALY DETECTION SUMMARY")
        logger.info("="*80)
        
        logger.info(f"\nüìä Anomalies Detected:")
        logger.info(f"   Total: {self.anomaly_report['total_count']:,}")
        logger.info(f"   Critical: {self.anomaly_report['critical_count']:,}")
        
        logger.info(f"\nüè∑Ô∏è  Flags Created: {len(self.anomaly_report['flags_created'])}")
        
        # Show flags created
        if self.anomaly_report['flags_created']:
            logger.info(f"\n   Flag columns created:")
            unique_flags = list(set(self.anomaly_report['flags_created']))
            for flag in unique_flags[:15]:
                logger.info(f"     - {flag}")
            if len(unique_flags) > 15:
                logger.info(f"     ... and {len(unique_flags) - 15} more")
        
        # Data integrity check
        original_cols = len(df.columns)
        final_cols = len(df_result.columns)
        flags_added = final_cols - original_cols
        
        logger.info(f"\nüìä Data Integrity:")
        logger.info(f"   Original columns: {original_cols}")
        logger.info(f"   Final columns: {final_cols}")
        logger.info(f"   Flags added: {flags_added}")
        logger.info(f"   Original rows: {len(df):,}")
        logger.info(f"   Final rows: {len(df_result):,}")
        logger.info(f"   ‚úì No rows removed (flag-only mode)")
        
        # Send alert
        self.send_alert()
        
        # Save report (local + GCS)
        self._save_report()
        
        return df_result, self.anomaly_report
    
    def _save_report(self):
        """Save anomaly report with proper JSON serialization (local + GCS)."""
        output_dir = Path("data/anomaly_reports")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save JSON report locally
        json_filename = f"anomaly_report_{self.dataset_name}_{timestamp}.json"
        json_path = output_dir / json_filename
        
        try:
            with open(json_path, 'w') as f:
                json.dump(self.anomaly_report, f, indent=2, cls=NumpyEncoder)
            
            logger.info(f"\nüíæ Anomaly report saved locally: {json_path}")
            
            # Upload JSON to GCS
            if self.use_gcs:
                json_content = json.dumps(self.anomaly_report, indent=2, cls=NumpyEncoder)
                self.upload_to_gcs(json_content, json_filename)
                
        except Exception as e:
            logger.error(f"   ‚ùå Failed to save JSON report: {e}")
        
        # Save CSV summary locally
        if self.anomaly_report.get('statistical_outliers'):
            csv_filename = f"outlier_summary_{self.dataset_name}_{timestamp}.csv"
            csv_path = output_dir / csv_filename
            
            try:
                outlier_df = pd.DataFrame(self.anomaly_report['statistical_outliers'])
                outlier_df.to_csv(csv_path, index=False)
                
                logger.info(f"üíæ Outlier summary saved locally: {csv_path}")
                
                # Upload CSV to GCS
                if self.use_gcs:
                    self.upload_csv_to_gcs(outlier_df, csv_filename, subfolder='data/anomaly_reports')
                    
            except Exception as e:
                logger.error(f"   ‚ùå Failed to save CSV summary: {e}")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Run anomaly detection with GCS integration."""
    
    # GCS configuration
    bucket_name = 'mlops-financial-stress-data'
    use_gcs = True
    
    # Initialize GCS client and download data
    if use_gcs:
        try:
            logger.info(f"Connecting to GCS bucket: gs://{bucket_name}/")
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            
            # Download features_engineered.csv from GCS
            blob_path = "data/processed/features_engineered.csv"
            blob = bucket.blob(blob_path)
            
            local_dir = Path("data/processed")
            local_dir.mkdir(parents=True, exist_ok=True)
            local_path = local_dir / "features_engineered.csv"
            
            logger.info(f"Downloading: gs://{bucket_name}/{blob_path}")
            blob.download_to_filename(str(local_path))
            
            size_mb = local_path.stat().st_size / (1024 * 1024)
            logger.info(f"‚úì Downloaded: {local_path} ({size_mb:.2f} MB)")
            
            filepath = local_path
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  GCS download failed: {e}")
            logger.info("Falling back to local file...")
            use_gcs = False
            filepath = Path("data/processed/features_engineered.csv")
    else:
        filepath = Path("data/processed/features_engineered.csv")
    
    if not filepath.exists():
        logger.error("‚ùå No features_engineered.csv found!")
        logger.error("Run feature engineering pipeline first")
        return
    
    logger.info(f"Loading: {filepath}")
    df = pd.read_csv(filepath)

    # Ensure Date is datetime
    if 'Date' in df.columns:
        if not pd.api.types.is_datetime64_any_dtype(df['Date']):
            df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')
        logger.info(f"   Date column: {df['Date'].dtype}")
    
    # Run detection
    detector = AnomalyDetectorQuarterly(
        dataset_name=filepath.stem,
        bucket_name=bucket_name,
        use_gcs=use_gcs
    )
    
    group_col = 'Company' if 'Company' in df.columns else None
    
    df_flagged, report = detector.run_detection(df, group_col=group_col)
    
    # Save output with flags (local + GCS)
    output_filename = f"{filepath.stem}_with_anomaly_flags.csv"
    output_path = Path("data/processed") / output_filename
    df_flagged.to_csv(output_path, index=False)
    
    logger.info(f"\n‚úì Saved flagged data locally: {output_path}")
    
    # Upload to GCS
    if use_gcs:
        detector.upload_csv_to_gcs(df_flagged, output_filename, subfolder='data/processed')
    
    logger.info(f"  Original shape: {df.shape}")
    logger.info(f"  Final shape: {df_flagged.shape}")
    logger.info(f"  Flags added: {df_flagged.shape[1] - df.shape[1]}")
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ ANOMALY DETECTION COMPLETE")
    logger.info("="*80)
    
    if report['critical_count'] > 0:
        logger.warning(f"\n‚ö†Ô∏è  {report['critical_count']:,} CRITICAL anomalies found!")
        logger.warning("   Review anomaly report before proceeding")
    else:
        logger.info("\n‚úÖ No critical anomalies detected")
    
    logger.info(f"\nüìÅ Output Locations:")
    logger.info(f"   Local data: {output_path}")
    logger.info(f"   Local reports: data/anomaly_reports/")
    if use_gcs:
        logger.info(f"   GCS data: gs://{bucket_name}/data/processed/{output_filename}")
        logger.info(f"   GCS reports: gs://{bucket_name}/data/anomaly_reports/")
    
    logger.info("\n‚û°Ô∏è  Next Steps:")
    logger.info(f"   1. Review: data/anomaly_reports/")
    logger.info(f"   2. Use: {output_path}")
    logger.info(f"   3. Next: python step7_drift_detection.py")


if __name__ == "__main__":
    main()